# Container Management API - Frontend Integration Guide

**Service:** AIWM
**Module:** Resources (Application Container & Inference Container)
**Base URL:** `http://localhost:3305` (development)
**Authentication:** Bearer Token (JWT)
**Version:** 1.0 (V1 - Metadata only, mock actions)
**Date:** 2025-12-03

---

## üìã T·ªïng quan

API qu·∫£n l√Ω 2 lo·∫°i containers:
1. **Application Container** - Docker containers cho user applications (Postgres, Nginx, Redis...)
2. **Inference Container** - Specialized containers cho AI model inference

**‚ö†Ô∏è L∆∞u √Ω V1:**
- CRUD operations ho·∫°t ƒë·ªông ƒë·∫ßy ƒë·ªß v·ªõi database
- Lifecycle actions (start/stop/restart/exec) tr·∫£ v·ªÅ **mock responses**
- Monitoring data (logs/metrics) l√† **mock data**
- **V2 s·∫Ω t√≠ch h·ª£p th·∫≠t v·ªõi Docker/Podman tr√™n worker nodes**

---

## üéØ Use Cases

### Application Container

| Use Case | M√¥ t·∫£ | Example |
|----------|-------|---------|
| **Databases** | PostgreSQL, MySQL, MongoDB | `postgres:16-alpine` |
| **Web Servers** | Nginx, Apache, Caddy | `nginx:alpine` |
| **Caching** | Redis, Memcached | `redis:7-alpine` |
| **Message Queues** | RabbitMQ, Kafka | `rabbitmq:management` |
| **Development Tools** | Code servers, Jupyter | `jupyter/tensorflow-notebook` |
| **Monitoring** | Prometheus, Grafana | `grafana/grafana:latest` |

### Inference Container

| Use Case | M√¥ t·∫£ | Example |
|----------|-------|---------|
| **LLM Inference** | Serve language models | Triton + LLaMA 3.1 |
| **Vision Models** | Image classification, detection | TensorFlow Serving + YOLO |
| **Voice Models** | Speech-to-text, TTS | Whisper large-v3 |
| **Embedding Models** | Sentence transformers | FastEmbed server |

---

## üìä Container Types Comparison

| Feature | Application Container | Inference Container |
|---------|----------------------|---------------------|
| **Purpose** | General apps | AI model serving |
| **Image Source** | Docker Hub, GHCR, Private | Custom inference images |
| **GPU Support** | Optional | Usually required |
| **Port Mapping** | Flexible | Fixed container port |
| **Volume Mounts** | Full support | Model path only |
| **Health Check** | Optional | Required |
| **Model Path** | N/A | Required (S3/local) |

---

## üîê Authentication

```http
Authorization: Bearer <your-jwt-token>
```

---

## üì¶ PART 1: APPLICATION CONTAINER

### API 1: Create Application Container

#### Endpoint
```
POST /resources
```

#### Request Body

```typescript
{
  name: string;                          // Container name
  description?: string;                  // Description
  resourceType: 'application-container'; // Fixed value
  nodeId: string;                        // Node ID to run container
  config: {
    type: 'application-container';       // Config discriminator
    registry: 'docker-hub' | 'ghcr' | 'private';
    imageName: string;                   // Image name
    imageTag?: string;                   // Image tag (default: latest)
    registryAuth?: {                     // For private registry
      username?: string,
      password?: string,
      token?: string
    },
    containerPorts?: [                   // Port mappings
      {
        containerPort: number,
        hostPort?: number,               // Auto-assign if not specified
        protocol?: 'tcp' | 'udp'
      }
    ],
    cpuCores?: number;                   // CPU limit
    ramLimit?: number;                   // RAM limit (GB)
    gpuDeviceIds?: string[];             // Optional GPU
    volumes?: [                          // Volume mounts
      {
        hostPath: string,
        containerPath: string,
        readOnly?: boolean
      }
    ],
    environment?: {                      // Environment variables
      [key: string]: string
    },
    networkMode?: 'bridge' | 'host' | 'none'
  }
}
```

#### Example: PostgreSQL Database

```bash
curl -X POST "http://localhost:3305/resources" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "PostgreSQL Production DB",
    "description": "Main PostgreSQL database for production",
    "resourceType": "application-container",
    "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
    "config": {
      "type": "application-container",
      "registry": "docker-hub",
      "imageName": "postgres",
      "imageTag": "16-alpine",
      "containerPorts": [
        {
          "containerPort": 5432,
          "hostPort": 5432,
          "protocol": "tcp"
        }
      ],
      "cpuCores": 4,
      "ramLimit": 8,
      "volumes": [
        {
          "hostPath": "/data/postgres/prod",
          "containerPath": "/var/lib/postgresql/data",
          "readOnly": false
        }
      ],
      "environment": {
        "POSTGRES_USER": "admin",
        "POSTGRES_PASSWORD": "secure-password-here",
        "POSTGRES_DB": "production"
      },
      "networkMode": "bridge"
    }
  }'
```

#### Example: Nginx Web Server

```json
{
  "name": "Nginx Reverse Proxy",
  "description": "Nginx as reverse proxy and load balancer",
  "resourceType": "application-container",
  "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
  "config": {
    "type": "application-container",
    "registry": "docker-hub",
    "imageName": "nginx",
    "imageTag": "alpine",
    "containerPorts": [
      { "containerPort": 80, "hostPort": 80 },
      { "containerPort": 443, "hostPort": 443 }
    ],
    "cpuCores": 2,
    "ramLimit": 2,
    "volumes": [
      {
        "hostPath": "/etc/nginx/conf.d",
        "containerPath": "/etc/nginx/conf.d",
        "readOnly": true
      },
      {
        "hostPath": "/var/log/nginx",
        "containerPath": "/var/log/nginx"
      }
    ],
    "networkMode": "host"
  }
}
```

#### Example: Redis Cache

```json
{
  "name": "Redis Cache",
  "description": "Redis for session storage and caching",
  "resourceType": "application-container",
  "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
  "config": {
    "type": "application-container",
    "registry": "docker-hub",
    "imageName": "redis",
    "imageTag": "7-alpine",
    "containerPorts": [
      { "containerPort": 6379, "hostPort": 6379 }
    ],
    "cpuCores": 2,
    "ramLimit": 4,
    "volumes": [
      {
        "hostPath": "/data/redis",
        "containerPath": "/data"
      }
    ],
    "environment": {
      "REDIS_PASSWORD": "redis-secure-password"
    }
  }
}
```

### Field Descriptions - Application Container

| Field | Type | Required | M√¥ t·∫£ | UI Component | Default |
|-------|------|----------|-------|--------------|---------|
| **Basic** ||||||
| `name` | string | ‚úÖ | Container name | Text input | - |
| `description` | string | ‚ùå | Description | Textarea | - |
| `nodeId` | string | ‚úÖ | Node ch·∫°y container | Select dropdown | - |
| **Image** ||||||
| `registry` | enum | ‚úÖ | Registry source | Radio buttons | `docker-hub` |
| `imageName` | string | ‚úÖ | Image name | Text input v·ªõi search | - |
| `imageTag` | string | ‚ùå | Image tag | Text input | `latest` |
| `registryAuth` | object | If private | Auth credentials | Collapsible section | - |
| **Ports** ||||||
| `containerPorts` | array | ‚ùå | Port mappings | Dynamic list | `[]` |
| `containerPorts[].containerPort` | number | ‚úÖ | Container port | Number input | - |
| `containerPorts[].hostPort` | number | ‚ùå | Host port | Number input | Auto-assign |
| `containerPorts[].protocol` | enum | ‚ùå | Protocol | Select | `tcp` |
| **Resources** ||||||
| `cpuCores` | number | ‚ùå | CPU cores limit | Number / Slider | No limit |
| `ramLimit` | number | ‚ùå | RAM limit (GB) | Number / Slider | No limit |
| `gpuDeviceIds` | array | ‚ùå | GPU devices | Multi-select | `[]` |
| **Storage** ||||||
| `volumes` | array | ‚ùå | Volume mounts | Dynamic list | `[]` |
| `volumes[].hostPath` | string | ‚úÖ | Host directory | Text / File browser | - |
| `volumes[].containerPath` | string | ‚úÖ | Container mount point | Text input | - |
| `volumes[].readOnly` | boolean | ‚ùå | Read-only mount | Checkbox | `false` |
| **Config** ||||||
| `environment` | object | ‚ùå | Environment vars | Key-value pairs editor | `{}` |
| `networkMode` | enum | ‚ùå | Network mode | Radio buttons | `bridge` |

### Registry Options

| Registry | Value | Icon | Authentication Required | Image Format |
|----------|-------|------|------------------------|--------------|
| **Docker Hub** | `docker-hub` | üê≥ | Only for private images | `postgres`, `nginx:alpine` |
| **GitHub Container Registry** | `ghcr` | üêô | Yes (Personal token) | `ghcr.io/owner/image:tag` |
| **Private Registry** | `private` | üîí | Yes (Username/password) | `registry.example.com/image:tag` |

### Network Modes

| Mode | Value | Use Case | IP Address | Port Binding |
|------|-------|----------|------------|--------------|
| **Bridge** | `bridge` | Default, isolated network | Container IP (172.17.x.x) | Map to host port |
| **Host** | `host` | Direct host network access | Host IP | Share host ports |
| **None** | `none` | No network (for security) | No network | No ports |

### Common Environment Variables

#### PostgreSQL
```typescript
{
  POSTGRES_USER: "admin",
  POSTGRES_PASSWORD: "***",
  POSTGRES_DB: "production",
  POSTGRES_INITDB_ARGS: "--encoding=UTF8"
}
```

#### MySQL
```typescript
{
  MYSQL_ROOT_PASSWORD: "***",
  MYSQL_DATABASE: "app_db",
  MYSQL_USER: "app_user",
  MYSQL_PASSWORD: "***"
}
```

#### Redis
```typescript
{
  REDIS_PASSWORD: "***",
  REDIS_MAXMEMORY: "4gb",
  REDIS_MAXMEMORY_POLICY: "allkeys-lru"
}
```

### UI Form - Application Container

```
‚îå‚îÄ Create Application Container ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Basic Information                                      ‚îÇ
‚îÇ ‚îú‚îÄ Name: [PostgreSQL Production DB_______________]    ‚îÇ
‚îÇ ‚îú‚îÄ Description: [Main database for...___________]     ‚îÇ
‚îÇ ‚îî‚îÄ Node: [worker-01 ‚ñº]                                ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Container Image                                        ‚îÇ
‚îÇ ‚îú‚îÄ Registry: ‚óè Docker Hub  ‚óã GitHub  ‚óã Private       ‚îÇ
‚îÇ ‚îú‚îÄ Image: [postgres_______________________] [Search]  ‚îÇ
‚îÇ ‚îî‚îÄ Tag: [16-alpine] (leave empty for 'latest')       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Port Mappings                         [+ Add Port]    ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ Container Port: [5432] ‚Üí Host Port: [5432]      ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Protocol: [TCP ‚ñº]                     [Remove]  ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Resources                                              ‚îÇ
‚îÇ ‚îú‚îÄ CPU Cores: [4 ‚îÅ‚îÅ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ] (no limit if empty)     ‚îÇ
‚îÇ ‚îú‚îÄ RAM Limit: [8 GB ‚îÅ‚îÅ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ] (no limit if empty)    ‚îÇ
‚îÇ ‚îî‚îÄ GPU: [ ] Enable GPU [Select Devices ‚ñº]            ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Volume Mounts                         [+ Add Volume]  ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ Host: [/data/postgres/prod______________]       ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Container: [/var/lib/postgresql/data____]       ‚îÇ   ‚îÇ
‚îÇ ‚îÇ [‚úì] Read-Only                         [Remove]  ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Environment Variables                 [+ Add Var]     ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ POSTGRES_USER    = [admin__________]   [Remove] ‚îÇ   ‚îÇ
‚îÇ ‚îÇ POSTGRES_PASSWORD= [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè______]   [Remove]  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ POSTGRES_DB      = [production_____]   [Remove] ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Network                                                ‚îÇ
‚îÇ ‚îî‚îÄ Mode: ‚óè Bridge  ‚óã Host  ‚óã None                    ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [Cancel]                           [Create Container] ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ü§ñ PART 2: INFERENCE CONTAINER

### API 2: Create Inference Container

#### Endpoint
```
POST /resources
```

#### Request Body

```typescript
{
  name: string;
  description?: string;
  resourceType: 'inference-container';
  nodeId: string;
  config: {
    type: 'inference-container';
    modelId: string;                     // Model ID from Model module
    modelPath: string;                   // S3 or local path to model
    dockerImage: string;                 // Inference server image
    containerPort: number;               // Inference endpoint port
    gpuDeviceIds: string[];              // Required GPU devices
    gpuMemoryLimit?: number;             // GPU memory limit (MB)
    cpuCores?: number;
    ramLimit?: number;
    environment?: {
      [key: string]: string
    },
    healthCheckPath?: string;            // Health check endpoint
  }
}
```

#### Example: Whisper Large V3 (Speech-to-Text)

```bash
curl -X POST "http://localhost:3305/resources" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Whisper Large V3 Inference",
    "description": "Speech-to-text model for transcription service",
    "resourceType": "inference-container",
    "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
    "config": {
      "type": "inference-container",
      "modelId": "67891234abcd5678ef901234",
      "modelPath": "s3://models/whisper-large-v3.tar.gz",
      "dockerImage": "nvcr.io/nvidia/tritonserver:24.01",
      "containerPort": 8000,
      "gpuDeviceIds": ["GPU-0"],
      "gpuMemoryLimit": 16384,
      "cpuCores": 4,
      "ramLimit": 16,
      "environment": {
        "MODEL_NAME": "whisper-large-v3",
        "MODEL_REPOSITORY": "/models",
        "LOG_LEVEL": "INFO"
      },
      "healthCheckPath": "/v2/health/ready"
    }
  }'
```

#### Example: LLaMA 3.1 70B (LLM Inference)

```json
{
  "name": "LLaMA 3.1 70B Inference",
  "description": "Large language model inference server",
  "resourceType": "inference-container",
  "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
  "config": {
    "type": "inference-container",
    "modelId": "67891234abcd5678ef901235",
    "modelPath": "s3://models/llama-3.1-70b.safetensors",
    "dockerImage": "vllm/vllm-openai:latest",
    "containerPort": 8000,
    "gpuDeviceIds": ["GPU-0", "GPU-1", "GPU-2", "GPU-3"],
    "gpuMemoryLimit": 81920,
    "cpuCores": 16,
    "ramLimit": 64,
    "environment": {
      "MODEL_NAME": "llama-3.1-70b",
      "TENSOR_PARALLEL_SIZE": "4",
      "MAX_MODEL_LEN": "4096",
      "GPU_MEMORY_UTILIZATION": "0.95"
    },
    "healthCheckPath": "/health"
  }
}
```

### Field Descriptions - Inference Container

| Field | Type | Required | M√¥ t·∫£ | UI Component |
|-------|------|----------|-------|--------------|
| **Basic** |||||
| `name` | string | ‚úÖ | Container name | Auto-generated from model name |
| `description` | string | ‚ùå | Description | Textarea |
| `nodeId` | string | ‚úÖ | Node with GPU | Select (filter nodes with GPU) |
| **Model** |||||
| `modelId` | string | ‚úÖ | Model ID | Select from Model list |
| `modelPath` | string | ‚úÖ | Model file path | Auto-filled from Model |
| `dockerImage` | string | ‚úÖ | Inference image | Select from templates |
| `containerPort` | number | ‚úÖ | Service port | Number input (default 8000) |
| **GPU** |||||
| `gpuDeviceIds` | array | ‚úÖ | GPU devices | Multi-select (required ‚â•1) |
| `gpuMemoryLimit` | number | ‚ùå | GPU memory (MB) | Number input |
| **Resources** |||||
| `cpuCores` | number | ‚ùå | CPU cores | Number input |
| `ramLimit` | number | ‚ùå | RAM (GB) | Number input |
| **Advanced** |||||
| `environment` | object | ‚ùå | Env vars | Key-value editor |
| `healthCheckPath` | string | ‚ùå | Health endpoint | Text input (default `/health`) |

### Inference Server Images

| Server | Docker Image | Supported Models | Features |
|--------|-------------|------------------|----------|
| **NVIDIA Triton** | `nvcr.io/nvidia/tritonserver:24.01` | TensorFlow, PyTorch, ONNX | Multi-model, dynamic batching |
| **vLLM** | `vllm/vllm-openai:latest` | LLMs (LLaMA, GPT, Mistral) | Fast inference, OpenAI API |
| **TensorRT-LLM** | `nvcr.io/nvidia/tensorrt-llm:24.01` | Optimized LLMs | Highest performance |
| **TensorFlow Serving** | `tensorflow/serving:latest-gpu` | TensorFlow models | Production-ready |
| **TorchServe** | `pytorch/torchserve:latest-gpu` | PyTorch models | Easy deployment |
| **FastAPI Custom** | `custom/inference-server:latest` | Any | Custom logic |

### Common Environment Variables - Inference

#### vLLM (LLMs)
```typescript
{
  MODEL_NAME: "llama-3.1-70b",
  TENSOR_PARALLEL_SIZE: "4",      // Number of GPUs
  MAX_MODEL_LEN: "4096",          // Context length
  GPU_MEMORY_UTILIZATION: "0.95", // GPU memory usage
  DTYPE: "auto"                   // float16, bfloat16, auto
}
```

#### Triton Inference Server
```typescript
{
  MODEL_NAME: "whisper-large-v3",
  MODEL_REPOSITORY: "/models",
  LOG_LEVEL: "INFO",
  STRICT_MODEL_CONFIG: "false",
  BACKEND: "pytorch"              // pytorch, tensorflow, onnx
}
```

### UI Form - Inference Container

```
‚îå‚îÄ Create Inference Container ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Select Model                                           ‚îÇ
‚îÇ ‚îî‚îÄ Model: [Whisper Large V3 ‚ñº]                        ‚îÇ
‚îÇ    Speech-to-text ¬∑ 1.5B parameters ¬∑ GPU Required    ‚îÇ
‚îÇ    Model Path: s3://models/whisper-large-v3.tar.gz    ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Deployment Configuration                               ‚îÇ
‚îÇ ‚îú‚îÄ Name: [Whisper Large V3 Inference_________]        ‚îÇ
‚îÇ ‚îú‚îÄ Node: [worker-gpu-01 ‚ñº] (4x NVIDIA A100 available)‚îÇ
‚îÇ ‚îî‚îÄ Inference Server: [NVIDIA Triton Server 24.01 ‚ñº]  ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ GPU Allocation                                         ‚îÇ
‚îÇ ‚îú‚îÄ Devices: [‚òë GPU-0] [ ] GPU-1 [ ] GPU-2 [ ] GPU-3 ‚îÇ
‚îÇ ‚îî‚îÄ Memory Limit: [16384 MB] (16 GB per GPU)          ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Resources                                              ‚îÇ
‚îÇ ‚îú‚îÄ CPU Cores: [4]                                     ‚îÇ
‚îÇ ‚îî‚îÄ RAM: [16 GB]                                       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Network                                                ‚îÇ
‚îÇ ‚îî‚îÄ Container Port: [8000] (default for most servers)  ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Advanced Settings                     [‚ñ∂ Expand]      ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [Cancel]                         [Deploy Inference ‚Üí] ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã API 3: List Containers

### Endpoint
```
GET /resources?resourceType=application-container
GET /resources?resourceType=inference-container
```

### Example Response - Application Container

```json
{
  "data": [
    {
      "_id": "675b2c3d4e5f6a7b8c9d0e0f",
      "name": "PostgreSQL Production DB",
      "description": "Main PostgreSQL database",
      "resourceType": "application-container",
      "nodeId": "675a1b2c3d4e5f6a7b8c9d0e",
      "status": "running",
      "config": {
        "type": "application-container",
        "registry": "docker-hub",
        "imageName": "postgres",
        "imageTag": "16-alpine",
        "containerPorts": [
          { "containerPort": 5432, "hostPort": 5432 }
        ],
        "cpuCores": 4,
        "ramLimit": 8
      },
      "runtime": {
        "id": "container-abc123def456",
        "endpoint": "192.168.100.5:5432",
        "allocatedCPU": 4,
        "allocatedRAM": 8,
        "startedAt": "2025-12-03T10:00:00.000Z"
      },
      "createdAt": "2025-12-03T09:00:00.000Z"
    }
  ],
  "pagination": { "total": 25, "page": 1, "limit": 10 }
}
```

### UI Layout - Container List

```
‚îå‚îÄ Application Containers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [+ New Container]  Filter: [All Types ‚ñº] [Running ‚ñº] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úÖ PostgreSQL Production DB           [‚ãÆ Actions ‚ñº]  ‚îÇ
‚îÇ    postgres:16-alpine ¬∑ 4 CPU ¬∑ 8 GB RAM             ‚îÇ
‚îÇ    192.168.100.5:5432 [üìã]  ¬∑ Running for 3h         ‚îÇ
‚îÇ    Node: worker-01                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úÖ Redis Cache                         [‚ãÆ Actions ‚ñº]  ‚îÇ
‚îÇ    redis:7-alpine ¬∑ 2 CPU ¬∑ 4 GB RAM                 ‚îÇ
‚îÇ    192.168.100.6:6379 [üìã]  ¬∑ Running for 5d         ‚îÇ
‚îÇ    Node: worker-01                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚èπÔ∏è Nginx Reverse Proxy                 [‚ãÆ Actions ‚ñº]  ‚îÇ
‚îÇ    nginx:alpine ¬∑ 2 CPU ¬∑ 2 GB RAM                   ‚îÇ
‚îÇ    Stopped 1h ago                                     ‚îÇ
‚îÇ    Node: worker-02                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ Inference Containers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [+ New Inference]  Filter: [All Models ‚ñº] [Running ‚ñº]‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úÖ Whisper Large V3 Inference          [‚ãÆ Actions ‚ñº]  ‚îÇ
‚îÇ    Triton Server ¬∑ 1x GPU-0 ¬∑ 4 CPU ¬∑ 16 GB          ‚îÇ
‚îÇ    192.168.100.10:8000 [üìã]  ¬∑ Running for 2h        ‚îÇ
‚îÇ    Model: Whisper Large V3 (1.5B params)              ‚îÇ
‚îÇ    Node: worker-gpu-01                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üîÑ LLaMA 3.1 70B Inference            [‚ãÆ Actions ‚ñº]  ‚îÇ
‚îÇ    vLLM ¬∑ 4x GPUs ¬∑ 16 CPU ¬∑ 64 GB                   ‚îÇ
‚îÇ    Deploying... 45%                                   ‚îÇ
‚îÇ    Model: LLaMA 3.1 70B (70B params)                  ‚îÇ
‚îÇ    Node: worker-gpu-02                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üé¨ API 4: Container Lifecycle & Monitoring

### 4.1 Start / Stop / Restart

Same as VM endpoints:
```bash
POST /resources/:id/start
POST /resources/:id/stop
POST /resources/:id/restart
```

### 4.2 Execute Command (Containers Only)

```bash
POST /resources/:id/exec
Content-Type: application/json

{
  "command": "ls -la /var/log",
  "workingDir": "/app"
}
```

**Response (V1 - Mock):**
```json
{
  "resourceId": "675b2c3d4e5f6a7b8c9d0e0f",
  "command": "ls -la /var/log",
  "exitCode": 0,
  "stdout": "total 48\ndrwxr-xr-x 5 root root 4096 Dec  3 10:00 .\n...",
  "stderr": "",
  "executedAt": "2025-12-03T12:00:00.000Z"
}
```

**UI - Container Shell:**
```
‚îå‚îÄ Container Shell: PostgreSQL Production DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ $ ‚ñà                                                    ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Commands History:                                      ‚îÇ
‚îÇ $ ls /var/lib/postgresql/data                         ‚îÇ
‚îÇ base  global  pg_hba.conf  pg_ident.conf  ...        ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ $ psql -U admin -d production                         ‚îÇ
‚îÇ psql (16.1)                                           ‚îÇ
‚îÇ Type "help" for help.                                  ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ production=#                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.3 Get Logs

```bash
GET /resources/:id/logs
```

**Response:**
```json
{
  "resourceId": "675b2c3d4e5f6a7b8c9d0e0f",
  "logs": [
    {
      "timestamp": "2025-12-03T10:00:00.000Z",
      "stream": "stdout",
      "message": "PostgreSQL Database directory appears to contain a database; Skipping initialization"
    },
    {
      "timestamp": "2025-12-03T10:00:05.000Z",
      "stream": "stdout",
      "message": "2025-12-03 10:00:05.123 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432"
    },
    {
      "timestamp": "2025-12-03T10:00:10.000Z",
      "stream": "stdout",
      "message": "2025-12-03 10:00:10.456 UTC [1] LOG:  database system is ready to accept connections"
    }
  ]
}
```

### 4.4 Get Metrics

Same response format as VM, but container-specific metrics.

---

## üé® UI Components

### Container Card Component

```tsx
interface ContainerCardProps {
  container: Container;
  onAction: (action: string, id: string) => void;
}

function ContainerCard({ container, onAction }: ContainerCardProps) {
  const isInference = container.resourceType === 'inference-container';

  return (
    <Card
      title={
        <Space>
          <Badge status={statusColor[container.status]} />
          {container.name}
        </Space>
      }
      extra={
        <Dropdown menu={{
          items: [
            { key: 'start', label: 'Start', disabled: container.status === 'running' },
            { key: 'stop', label: 'Stop', disabled: container.status === 'stopped' },
            { key: 'restart', label: 'Restart' },
            { key: 'logs', label: 'View Logs' },
            { key: 'shell', label: 'Open Shell', disabled: isInference },
            { type: 'divider' },
            { key: 'delete', label: 'Delete', danger: true },
          ],
          onClick: ({ key }) => onAction(key, container._id)
        }}>
          <Button icon={<MoreOutlined />} />
        </Dropdown>
      }
    >
      {isInference ? (
        <InferenceInfo container={container} />
      ) : (
        <ApplicationInfo container={container} />
      )}
    </Card>
  );
}
```

### Form Validation

```typescript
const appContainerSchema = z.object({
  name: z.string().min(1).max(100),
  nodeId: z.string(),
  config: z.object({
    type: z.literal('application-container'),
    registry: z.enum(['docker-hub', 'ghcr', 'private']),
    imageName: z.string().min(1),
    imageTag: z.string().optional(),
    containerPorts: z.array(z.object({
      containerPort: z.number().min(1).max(65535),
      hostPort: z.number().min(1).max(65535).optional(),
      protocol: z.enum(['tcp', 'udp']).optional()
    })).optional(),
    volumes: z.array(z.object({
      hostPath: z.string().min(1),
      containerPath: z.string().min(1),
      readOnly: z.boolean().optional()
    })).optional(),
    environment: z.record(z.string()).optional()
  })
});

const inferenceContainerSchema = z.object({
  name: z.string().min(1).max(100),
  nodeId: z.string(),
  config: z.object({
    type: z.literal('inference-container'),
    modelId: z.string(),
    modelPath: z.string(),
    dockerImage: z.string(),
    containerPort: z.number(),
    gpuDeviceIds: z.array(z.string()).min(1, "At least one GPU required")
  })
});
```

---

## üìù Best Practices

### 1. Container Naming

```typescript
// Auto-generate names from image
function suggestContainerName(imageName: string, imageTag?: string): string {
  const tag = imageTag && imageTag !== 'latest' ? `-${imageTag}` : '';
  return `${imageName}${tag}-${Date.now().toString(36)}`;
}

// Examples:
// postgres:16-alpine ‚Üí postgres-16-alpine-abc123
// nginx ‚Üí nginx-xyz789
```

### 2. Port Conflict Detection

```typescript
async function checkPortAvailability(nodeId: string, port: number) {
  const containers = await fetchContainers({ nodeId, status: 'running' });
  const usedPorts = containers.flatMap(c =>
    c.config.containerPorts?.map(p => p.hostPort) ?? []
  );

  if (usedPorts.includes(port)) {
    throw new Error(`Port ${port} already in use on this node`);
  }
}
```

### 3. Resource Validation

```typescript
async function validateResources(nodeId: string, cpuCores?: number, ramLimit?: number, gpuDeviceIds?: string[]) {
  const node = await fetchNode(nodeId);

  if (cpuCores && cpuCores > node.cpuCores) {
    throw new Error(`Node only has ${node.cpuCores} CPU cores available`);
  }

  if (ramLimit && ramLimit > node.ramTotal / 1024) {
    throw new Error(`Node only has ${node.ramTotal / 1024} GB RAM available`);
  }

  if (gpuDeviceIds && gpuDeviceIds.length > 0) {
    const availableGPUs = node.gpuDevices?.map(g => g.deviceId) ?? [];
    const invalidGPUs = gpuDeviceIds.filter(id => !availableGPUs.includes(id));

    if (invalidGPUs.length > 0) {
      throw new Error(`Invalid GPU devices: ${invalidGPUs.join(', ')}`);
    }
  }
}
```

### 4. Image Search Integration

```typescript
// Integrate with Docker Hub API
async function searchDockerImages(query: string) {
  const response = await fetch(
    `https://hub.docker.com/v2/search/repositories/?query=${query}&page_size=25`
  );
  const data = await response.json();

  return data.results.map(repo => ({
    name: repo.repo_name,
    description: repo.short_description,
    stars: repo.star_count,
    official: repo.is_official
  }));
}
```

---

## ‚ö†Ô∏è V1 Limitations

1. **Mock Actions** - Start/stop/restart/exec ch·ªâ update DB
2. **No Real Deployment** - Containers kh√¥ng th·ª±c s·ª± ƒë∆∞·ª£c deploy
3. **Mock Logs** - Logs l√† fake data
4. **No Real-time Stats** - Metrics l√† mock data
5. **No Image Validation** - Kh√¥ng verify image existence tr∆∞·ªõc khi create

---

**Last Updated:** 2025-12-03
**Version:** 1.0 (V1 - Metadata Only)
**Status:** ‚úÖ Ready for Frontend Integration
